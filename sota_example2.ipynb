{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rohneju8sIj3"
   },
   "source": [
    "This notebook is based on the DQN example from TensorFlow, which can be found in [this repo](https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb). I've adapted it a bit to suit the AI Classroom training. This example uses the [TF-Agents](https://github.com/tensorflow/agents) RL framework, and more examples can be found in the GitHub repo for that project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbdQ5aIkrrCB"
   },
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZysfKeqltlUi",
    "outputId": "f01ab295-f604-4a7c-f935-5422d1db01e9"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install -y xvfb ffmpeg x11-utils\n",
    "!pip install 'gym==0.10.11'\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install PILLOW\n",
    "!pip install 'pyglet==1.3.2'\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install --upgrade tensorflow-probability\n",
    "!pip install tf-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWzLFrbcsh56"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tPUa3U34sjX5"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EsHgykTuMAL"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2e6_N_1Ltm-g"
   },
   "outputs": [],
   "source": [
    "# We will use a \"virtual environment\" in this example. Here we are going to initialize\n",
    "# a display object that will help us render OpenAI gym environments.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2XV7fu2tq7p"
   },
   "outputs": [],
   "source": [
    "# Load the \"Cart Pole\" environment, which already exists within TF-Agents. You\n",
    "# can also create this directly with something like OpenAI gym.\n",
    "env_name = 'CartPole-v0'\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "zIxMKgM1t4CB",
    "outputId": "664484e1-7e01-4532-8120-e3bbdd7676f9"
   },
   "outputs": [],
   "source": [
    "# Visualize a state of the environment. This environment is just a cart that\n",
    "# rides along a rail. A pole is balanced on top of the cart, and our job with RL\n",
    "# will be to create a policy that takes actions (moving the cart left and right)\n",
    "# to keep the pole balanced on the cart.\n",
    "train_py_env.reset()\n",
    "PIL.Image.fromarray(train_py_env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMwKfiDBubWT"
   },
   "outputs": [],
   "source": [
    "# Converts these environment to tensor representations to make them compatible \n",
    "# with Tensorflow-based agents.\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5Uzg5H1vRUH"
   },
   "source": [
    "# Define our Agent and some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcL9QdzjvOM4"
   },
   "outputs": [],
   "source": [
    "# Number of iterations will be the number of steps of our training that we will\n",
    "# complete where each step generated actions and state transitions. These \n",
    "# transitions get sent to a replay memory/ buffer for optimizing the policy\n",
    "# network.\n",
    "num_iterations = 20000 \n",
    "\n",
    "# An initial number of transition collection steps to complete to fill out the\n",
    "# memory buffer.\n",
    "initial_collect_steps = 1000 \n",
    "\n",
    "# After the initial collection, collect 1 transition per iteration.\n",
    "collect_steps_per_iteration = 1\n",
    "\n",
    "# The maximum length of our memory buffer.\n",
    "replay_buffer_max_length = 100000 \n",
    "\n",
    "# Update the policy network using batches of 64 transitions sampled from the\n",
    "# replay buffer.\n",
    "batch_size = 64 \n",
    "learning_rate = 1e-3 \n",
    "log_interval = 200 \n",
    "\n",
    "# Number of episodes to use for evaluation.\n",
    "num_eval_episodes = 10 \n",
    "eval_interval = 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3tzP7chvjlU"
   },
   "outputs": [],
   "source": [
    "# Define our deep Q-learning network.\n",
    "fc_layer_params = (100,)\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njof1kJTvkOC"
   },
   "outputs": [],
   "source": [
    "# Define some of the details of our agent for training purposes.\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "# Initialize the agent.\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9pt0jAEv_jr"
   },
   "outputs": [],
   "source": [
    "# Define a function we will use to evaluate the policy network. A common evaluation\n",
    "# metric is the the sum of rewards obtained while running a policy in an \n",
    "# environment for an episode. In this case, our episode runs for 200 steps/iterations,\n",
    "# so the max return is 200.\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUlMltbevrfQ"
   },
   "source": [
    "# Initialize the Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jU4KgBZjvo7j"
   },
   "outputs": [],
   "source": [
    "# We will only use the policy network based on the DQN during evaluation and\n",
    "# deployment of the resulting model.\n",
    "eval_policy = agent.policy\n",
    "\n",
    "# However, we will use a different policy during the exploration of the space.\n",
    "collect_policy = agent.collect_policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWn4VVcOwIR9"
   },
   "source": [
    "# Create the Replay/ Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tcj10sL7v0ln"
   },
   "outputs": [],
   "source": [
    "# Create the replay buffer.\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCHfPNtN2C68"
   },
   "outputs": [],
   "source": [
    "# Define some convenience functions we will use in filling the replay buffer.\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdJjlN082Pq_"
   },
   "outputs": [],
   "source": [
    "# Collect data from an initial number of steps to seed the replay buffer.\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "toegxDlf2QMp",
    "outputId": "87160db4-e178-40f4-bfab-1049c9e6c267"
   },
   "outputs": [],
   "source": [
    "# Create a dataset using the replay_buffer, which allow us to iterate over and\n",
    "# sample from the replay buffer.\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlMi8Ih82Zjs"
   },
   "source": [
    "# Train the Agent policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HN4pbef62Y0S",
    "outputId": "d04f3737-dcac-4a76-d387-ec980a238e5d"
   },
   "outputs": [],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step.\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ytLMNdvP2uy2"
   },
   "source": [
    "# Visualize our evaluation results and the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "ybrptmxe2oGK",
    "outputId": "cbe5e2af-c02e-4000-c52d-a0985bd406f5"
   },
   "outputs": [],
   "source": [
    "# Plot our return vs. the number of iterations in our training.\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKICrwwB21ob"
   },
   "outputs": [],
   "source": [
    "# Create a convenience function to help us embed videos of our policy at work.\n",
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "uqBf-vmc24Yb",
    "outputId": "daf52ead-575d-4547-97b4-2f8055375b0d"
   },
   "outputs": [],
   "source": [
    "# Create a video using our evaluation environment.\n",
    "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = eval_env.reset()\n",
    "      video.append_data(eval_py_env.render())\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        video.append_data(eval_py_env.render())\n",
    "  return embed_mp4(filename)\n",
    "\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "RrNqiCMx7vyn",
    "outputId": "dd0a81b5-f39d-4156-dee8-8a1c79dfd27c"
   },
   "outputs": [],
   "source": [
    "# See if our above policy works qualitative differently than random actions.\n",
    "create_policy_eval_video(random_policy, \"random-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhgIQEKRFfII"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sota_example2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
